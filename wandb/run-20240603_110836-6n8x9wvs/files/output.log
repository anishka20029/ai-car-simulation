Epoch 0/1000, Train Loss: 2.0268014669418335, Val Loss: 0.6237733960151672, Train Acc: 0.525, Val Acc: 1.0
Epoch 10/1000, Train Loss: 2.1746609210968018, Val Loss: 0.5310426354408264, Train Acc: 0.475, Val Acc: 0.8999999761581421
Epoch 20/1000, Train Loss: 2.7860002517700195, Val Loss: 0.4157182574272156, Train Acc: 0.525, Val Acc: 0.8999999761581421
Epoch 30/1000, Train Loss: 1.3030575513839722, Val Loss: 0.340341717004776, Train Acc: 0.7, Val Acc: 0.8999999761581421
Epoch 40/1000, Train Loss: 1.1566476374864578, Val Loss: 0.2711043953895569, Train Acc: 0.7, Val Acc: 0.8999999761581421
Epoch 50/1000, Train Loss: 2.094401478767395, Val Loss: 0.23696450889110565, Train Acc: 0.5, Val Acc: 0.8999999761581421
Epoch 60/1000, Train Loss: 0.9001231491565704, Val Loss: 0.19891151785850525, Train Acc: 0.8, Val Acc: 0.8999999761581421
Epoch 70/1000, Train Loss: 0.9502415657043457, Val Loss: 0.18112559616565704, Train Acc: 0.725, Val Acc: 0.8999999761581421
Epoch 80/1000, Train Loss: 0.8185358345508575, Val Loss: 0.16565610468387604, Train Acc: 0.8, Val Acc: 0.8999999761581421
Epoch 90/1000, Train Loss: 1.0507723093032837, Val Loss: 0.16216999292373657, Train Acc: 0.7, Val Acc: 0.8999999761581421
Epoch 100/1000, Train Loss: 0.9087367653846741, Val Loss: 0.15950065851211548, Train Acc: 0.8, Val Acc: 0.8999999761581421
Epoch 110/1000, Train Loss: 1.02714803814888, Val Loss: 0.16237109899520874, Train Acc: 0.8, Val Acc: 0.8999999761581421
Epoch 120/1000, Train Loss: 1.2854977250099182, Val Loss: 0.16157224774360657, Train Acc: 0.75, Val Acc: 0.8999999761581421
Epoch 130/1000, Train Loss: 0.809428334236145, Val Loss: 0.14705756306648254, Train Acc: 0.7, Val Acc: 0.8999999761581421
Epoch 140/1000, Train Loss: 0.8612167239189148, Val Loss: 0.1557164192199707, Train Acc: 0.85, Val Acc: 0.8999999761581421
Epoch 150/1000, Train Loss: 0.975641518831253, Val Loss: 0.14729678630828857, Train Acc: 0.9, Val Acc: 0.8999999761581421
Epoch 160/1000, Train Loss: 1.204982966184616, Val Loss: 0.15212741494178772, Train Acc: 0.825, Val Acc: 0.8999999761581421
Epoch 170/1000, Train Loss: 1.5051780939102173, Val Loss: 0.15027591586112976, Train Acc: 0.7, Val Acc: 0.8999999761581421
Epoch 180/1000, Train Loss: 1.3073042035102844, Val Loss: 0.15834800899028778, Train Acc: 0.725, Val Acc: 0.8999999761581421
Epoch 190/1000, Train Loss: 0.7290890514850616, Val Loss: 0.14609912037849426, Train Acc: 0.775, Val Acc: 0.8999999761581421
Epoch 200/1000, Train Loss: 0.794331818819046, Val Loss: 0.149457186460495, Train Acc: 0.825, Val Acc: 0.8999999761581421
Epoch 210/1000, Train Loss: 0.9997037649154663, Val Loss: 0.15055683255195618, Train Acc: 0.85, Val Acc: 0.8999999761581421
Epoch 220/1000, Train Loss: 0.49655839800834656, Val Loss: 0.15031102299690247, Train Acc: 0.875, Val Acc: 0.8999999761581421
Epoch 230/1000, Train Loss: 0.5807573348283768, Val Loss: 0.15987148880958557, Train Acc: 0.8, Val Acc: 0.8999999761581421
Epoch 240/1000, Train Loss: 1.3953927755355835, Val Loss: 0.1522914171218872, Train Acc: 0.725, Val Acc: 0.8999999761581421
Epoch 250/1000, Train Loss: 0.7086722254753113, Val Loss: 0.14824947714805603, Train Acc: 0.875, Val Acc: 0.8999999761581421
Epoch 260/1000, Train Loss: 0.41579996794462204, Val Loss: 0.15091127157211304, Train Acc: 0.85, Val Acc: 0.8999999761581421
Epoch 270/1000, Train Loss: 0.5558395087718964, Val Loss: 0.13707585632801056, Train Acc: 0.875, Val Acc: 0.8999999761581421
Epoch 280/1000, Train Loss: 0.4380183815956116, Val Loss: 0.14281904697418213, Train Acc: 0.875, Val Acc: 0.8999999761581421
Epoch 290/1000, Train Loss: 0.8826012015342712, Val Loss: 0.14866040647029877, Train Acc: 0.7, Val Acc: 0.8999999761581421
Epoch 300/1000, Train Loss: 0.9258177280426025, Val Loss: 0.14393678307533264, Train Acc: 0.75, Val Acc: 0.8999999761581421
Epoch 310/1000, Train Loss: 0.7603824436664581, Val Loss: 0.1451614797115326, Train Acc: 0.85, Val Acc: 0.8999999761581421
Epoch 320/1000, Train Loss: 0.3089301362633705, Val Loss: 0.14352652430534363, Train Acc: 0.925, Val Acc: 0.8999999761581421
Epoch 330/1000, Train Loss: 0.45150163769721985, Val Loss: 0.15512947738170624, Train Acc: 0.875, Val Acc: 0.8999999761581421
Epoch 340/1000, Train Loss: 0.5337534546852112, Val Loss: 0.15473225712776184, Train Acc: 0.875, Val Acc: 0.8999999761581421
Epoch 350/1000, Train Loss: 0.8370014727115631, Val Loss: 0.15824511647224426, Train Acc: 0.825, Val Acc: 0.8999999761581421
Epoch 360/1000, Train Loss: 0.8493798971176147, Val Loss: 0.16580446064472198, Train Acc: 0.825, Val Acc: 0.8999999761581421
Epoch 370/1000, Train Loss: 0.5346065610647202, Val Loss: 0.15204647183418274, Train Acc: 0.8, Val Acc: 0.8999999761581421
Epoch 380/1000, Train Loss: 0.6217387616634369, Val Loss: 0.15694348514080048, Train Acc: 0.85, Val Acc: 0.8999999761581421
Epoch 390/1000, Train Loss: 0.7852509915828705, Val Loss: 0.15323445200920105, Train Acc: 0.85, Val Acc: 0.8999999761581421
Epoch 400/1000, Train Loss: 0.7889219224452972, Val Loss: 0.15041057765483856, Train Acc: 0.85, Val Acc: 0.8999999761581421
Epoch 410/1000, Train Loss: 0.7048293948173523, Val Loss: 0.1522945910692215, Train Acc: 0.875, Val Acc: 0.8999999761581421
Epoch 420/1000, Train Loss: 0.3926757127046585, Val Loss: 0.16325588524341583, Train Acc: 0.9, Val Acc: 0.8999999761581421
Epoch 430/1000, Train Loss: 0.9681125581264496, Val Loss: 0.15157870948314667, Train Acc: 0.8, Val Acc: 0.8999999761581421
Epoch 440/1000, Train Loss: 0.5741147696971893, Val Loss: 0.14592182636260986, Train Acc: 0.875, Val Acc: 0.8999999761581421
Epoch 450/1000, Train Loss: 0.6741766631603241, Val Loss: 0.15333090722560883, Train Acc: 0.85, Val Acc: 0.8999999761581421
Epoch 460/1000, Train Loss: 0.524815171957016, Val Loss: 0.15581676363945007, Train Acc: 0.875, Val Acc: 0.8999999761581421
Epoch 470/1000, Train Loss: 1.0861794650554657, Val Loss: 0.17220094799995422, Train Acc: 0.725, Val Acc: 0.8999999761581421
Epoch 480/1000, Train Loss: 1.624945506453514, Val Loss: 0.20340421795845032, Train Acc: 0.8, Val Acc: 0.8999999761581421
Epoch 490/1000, Train Loss: 0.5482734739780426, Val Loss: 0.1738695502281189, Train Acc: 0.925, Val Acc: 0.8999999761581421
Epoch 500/1000, Train Loss: 0.41678304970264435, Val Loss: 0.15194568037986755, Train Acc: 0.925, Val Acc: 0.8999999761581421
Epoch 510/1000, Train Loss: 0.26923300698399544, Val Loss: 0.15794792771339417, Train Acc: 0.95, Val Acc: 0.8999999761581421
Epoch 520/1000, Train Loss: 0.291634738445282, Val Loss: 0.14548900723457336, Train Acc: 0.925, Val Acc: 0.8999999761581421
Epoch 530/1000, Train Loss: 0.6863848865032196, Val Loss: 0.15265041589736938, Train Acc: 0.825, Val Acc: 0.8999999761581421
Epoch 540/1000, Train Loss: 0.3493584841489792, Val Loss: 0.1503107249736786, Train Acc: 0.925, Val Acc: 0.8999999761581421
Epoch 550/1000, Train Loss: 0.33504995703697205, Val Loss: 0.14513644576072693, Train Acc: 0.95, Val Acc: 0.8999999761581421
Epoch 560/1000, Train Loss: 0.6415474116802216, Val Loss: 0.15427717566490173, Train Acc: 0.825, Val Acc: 0.8999999761581421
Epoch 570/1000, Train Loss: 0.3689913973212242, Val Loss: 0.16267935931682587, Train Acc: 0.95, Val Acc: 0.8999999761581421
Epoch 580/1000, Train Loss: 0.6950496137142181, Val Loss: 0.13971677422523499, Train Acc: 0.8, Val Acc: 0.8999999761581421
Epoch 590/1000, Train Loss: 0.48213905096054077, Val Loss: 0.1642039716243744, Train Acc: 0.875, Val Acc: 0.8999999761581421
Epoch 600/1000, Train Loss: 0.45932088792324066, Val Loss: 0.15511514246463776, Train Acc: 0.9, Val Acc: 0.8999999761581421
Epoch 610/1000, Train Loss: 0.46791309118270874, Val Loss: 0.14542236924171448, Train Acc: 0.875, Val Acc: 0.8999999761581421
Epoch 620/1000, Train Loss: 0.3482526168227196, Val Loss: 0.14469072222709656, Train Acc: 0.95, Val Acc: 0.8999999761581421
Epoch 630/1000, Train Loss: 0.9277941286563873, Val Loss: 0.17096813023090363, Train Acc: 0.825, Val Acc: 0.8999999761581421
Epoch 640/1000, Train Loss: 0.7408407330513, Val Loss: 0.1497311294078827, Train Acc: 0.85, Val Acc: 0.8999999761581421
Epoch 650/1000, Train Loss: 0.61863674223423, Val Loss: 0.1519276350736618, Train Acc: 0.9, Val Acc: 0.8999999761581421
Epoch 660/1000, Train Loss: 0.3589955121278763, Val Loss: 0.15596170723438263, Train Acc: 0.9, Val Acc: 0.8999999761581421
Epoch 670/1000, Train Loss: 0.5547856986522675, Val Loss: 0.1489381343126297, Train Acc: 0.875, Val Acc: 0.8999999761581421
Epoch 680/1000, Train Loss: 0.502103328704834, Val Loss: 0.1683204472064972, Train Acc: 0.875, Val Acc: 0.8999999761581421
Epoch 690/1000, Train Loss: 0.4844744950532913, Val Loss: 0.15301735699176788, Train Acc: 0.85, Val Acc: 0.8999999761581421
Epoch 700/1000, Train Loss: 0.5664587914943695, Val Loss: 0.14650920033454895, Train Acc: 0.85, Val Acc: 0.8999999761581421
Epoch 710/1000, Train Loss: 0.34592705219984055, Val Loss: 0.15406595170497894, Train Acc: 0.875, Val Acc: 0.8999999761581421
Epoch 720/1000, Train Loss: 0.5770642459392548, Val Loss: 0.14737018942832947, Train Acc: 0.85, Val Acc: 0.8999999761581421
Epoch 730/1000, Train Loss: 0.5887875109910965, Val Loss: 0.13489675521850586, Train Acc: 0.875, Val Acc: 0.8999999761581421
Epoch 740/1000, Train Loss: 0.17685434222221375, Val Loss: 0.1527191400527954, Train Acc: 1.0, Val Acc: 0.8999999761581421
Epoch 750/1000, Train Loss: 0.6608213782310486, Val Loss: 0.14946381747722626, Train Acc: 0.85, Val Acc: 0.8999999761581421
Epoch 760/1000, Train Loss: 0.438971683382988, Val Loss: 0.1434725821018219, Train Acc: 0.875, Val Acc: 0.8999999761581421
Epoch 770/1000, Train Loss: 0.410768486559391, Val Loss: 0.1464635729789734, Train Acc: 0.9, Val Acc: 0.8999999761581421
Epoch 780/1000, Train Loss: 0.3681381046772003, Val Loss: 0.1537989377975464, Train Acc: 0.925, Val Acc: 0.8999999761581421
Epoch 790/1000, Train Loss: 0.319015771150589, Val Loss: 0.1463405191898346, Train Acc: 1.0, Val Acc: 0.8999999761581421
Epoch 800/1000, Train Loss: 0.25759751349687576, Val Loss: 0.15530996024608612, Train Acc: 0.95, Val Acc: 0.8999999761581421
Epoch 810/1000, Train Loss: 0.44606176018714905, Val Loss: 0.15734362602233887, Train Acc: 0.925, Val Acc: 0.8999999761581421
Epoch 820/1000, Train Loss: 0.27992789819836617, Val Loss: 0.1402636170387268, Train Acc: 0.9, Val Acc: 0.8999999761581421
Epoch 830/1000, Train Loss: 0.8519347608089447, Val Loss: 0.14643768966197968, Train Acc: 0.875, Val Acc: 0.8999999761581421
Epoch 840/1000, Train Loss: 0.845879316329956, Val Loss: 0.1645587980747223, Train Acc: 0.825, Val Acc: 0.8999999761581421
Epoch 850/1000, Train Loss: 0.7031318545341492, Val Loss: 0.15589496493339539, Train Acc: 0.825, Val Acc: 0.8999999761581421
Epoch 860/1000, Train Loss: 0.7200784683227539, Val Loss: 0.14656075835227966, Train Acc: 0.775, Val Acc: 0.8999999761581421
Epoch 870/1000, Train Loss: 0.6249392926692963, Val Loss: 0.15145406126976013, Train Acc: 0.875, Val Acc: 0.8999999761581421
Epoch 880/1000, Train Loss: 0.25460024178028107, Val Loss: 0.14232034981250763, Train Acc: 0.9, Val Acc: 0.8999999761581421
Epoch 890/1000, Train Loss: 0.26028167456388474, Val Loss: 0.14953480660915375, Train Acc: 0.925, Val Acc: 0.8999999761581421
Epoch 900/1000, Train Loss: 0.15422940999269485, Val Loss: 0.14930459856987, Train Acc: 0.95, Val Acc: 0.8999999761581421
Epoch 910/1000, Train Loss: 0.4925498887896538, Val Loss: 0.14631357789039612, Train Acc: 0.95, Val Acc: 0.8999999761581421
Epoch 920/1000, Train Loss: 0.3841528594493866, Val Loss: 0.14523768424987793, Train Acc: 0.9, Val Acc: 0.8999999761581421
Epoch 930/1000, Train Loss: 0.3370363116264343, Val Loss: 0.13895699381828308, Train Acc: 0.925, Val Acc: 0.8999999761581421
Epoch 940/1000, Train Loss: 0.5359821021556854, Val Loss: 0.14571157097816467, Train Acc: 0.875, Val Acc: 0.8999999761581421
Epoch 950/1000, Train Loss: 0.4011055827140808, Val Loss: 0.15876677632331848, Train Acc: 0.9, Val Acc: 0.8999999761581421
Epoch 960/1000, Train Loss: 0.33352920413017273, Val Loss: 0.14030395448207855, Train Acc: 0.9, Val Acc: 0.8999999761581421
Epoch 970/1000, Train Loss: 0.5741888135671616, Val Loss: 0.14614567160606384, Train Acc: 0.925, Val Acc: 0.8999999761581421
Epoch 980/1000, Train Loss: 0.5081037729978561, Val Loss: 0.17275336384773254, Train Acc: 0.9, Val Acc: 0.8999999761581421
Epoch 990/1000, Train Loss: 0.9192522466182709, Val Loss: 0.18039920926094055, Train Acc: 0.85, Val Acc: 0.8999999761581421
[34m[1mwandb[39m[22m: [33mWARNING[39m Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'Train Loss': 2.0268014669418335, 'Validation Loss': 0.6237733960151672, 'Train Accuracy': 0.525, 'Validation Accuracy': 1.0, '_timestamp': 1717427320.725899}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/fc1.weight': {'_type': 'histogram', 'values': [1451, 1772, 1822, 1789, 1735, 1808, 1833, 1871, 1832, 1814, 1830, 1721, 1735, 1878, 1822, 1918, 1866, 1748, 1803, 1861, 1793, 1797, 1804, 1823, 1720, 1870, 1763, 1852, 1806, 1819, 1807, 1820, 1781, 1826, 1724, 1723, 1787, 1782, 1860, 1839, 1762, 1878, 1828, 1813, 1784, 1799, 1787, 1826, 1753, 1887, 1829, 1791, 1798, 1852, 1837, 1761, 1855, 1778, 1779, 1808, 1840, 1862, 1836, 1552], 'bins': [-0.10861257463693619, -0.10521812736988068, -0.10182367265224457, -0.09842922538518906, -0.09503477066755295, -0.09164032340049744, -0.08824586868286133, -0.08485142141580582, -0.08145696669816971, -0.0780625194311142, -0.07466806471347809, -0.07127361744642258, -0.06787916272878647, -0.06448471546173096, -0.06109026074409485, -0.05769580975174904, -0.05430135875940323, -0.05090691149234772, -0.04751246050000191, -0.0441180095076561, -0.04072355851531029, -0.03732910752296448, -0.03393465653061867, -0.03054020367562771, -0.027145754545927048, -0.023751303553581238, -0.020356852561235428, -0.016962401568889618, -0.013567950576543808, -0.010173499584197998, -0.006779048591852188, -0.003384597599506378, 9.853392839431763e-06, 0.0034043043851852417, 0.006798755377531052, 0.010193206369876862, 0.013587657362222672, 0.01698210835456848, 0.02037655934691429, 0.0237710103392601, 0.02716546133160591, 0.030559910461306572, 0.03395436331629753, 0.03734881430864334, 0.04074326530098915, 0.04413771629333496, 0.04753216728568077, 0.05092661827802658, 0.05432106554508209, 0.0577155165374279, 0.06110996752977371, 0.06450442224740982, 0.06789886951446533, 0.07129332423210144, 0.07468777149915695, 0.07808222621679306, 0.08147667348384857, 0.08487112820148468, 0.08826557546854019, 0.0916600301861763, 0.09505447745323181, 0.09844893217086792, 0.10184337943792343, 0.10523783415555954, 0.10863228142261505]}, '_timestamp': 1717427320.73019}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/fc1.bias': {'_type': 'histogram', 'values': [2, 2, 2, 1, 2, 1, 3, 1, 3, 3, 4, 0, 3, 1, 2, 2, 4, 5, 1, 2, 4, 0, 2, 0, 3, 2, 1, 1, 1, 1, 0, 2, 2, 4, 0, 6, 1, 1, 5, 1, 4, 1, 3, 2, 2, 1, 0, 5, 1, 0, 1, 4, 0, 1, 1, 2, 2, 1, 3, 1, 3, 6, 1, 2], 'bins': [-0.03311385586857796, -0.03208814933896065, -0.03106244094669819, -0.03003673255443573, -0.02901102416217327, -0.02798531763255596, -0.026959609240293503, -0.025933900848031044, -0.024908194318413734, -0.023882485926151276, -0.022856777533888817, -0.021831071004271507, -0.02080536261200905, -0.01977965421974659, -0.01875394769012928, -0.01772823929786682, -0.01670253276824951, -0.015676824375987053, -0.014651115983724594, -0.01362540852278471, -0.012599701061844826, -0.011573992669582367, -0.010548285208642483, -0.009522577747702599, -0.00849686935544014, -0.007471161894500256, -0.006445454433560371, -0.0054197465069592, -0.004394038580358028, -0.0033683311194181442, -0.0023426231928169727, -0.001316915499046445, -0.00029120780527591705, 0.0007344999467022717, 0.0017602076986804605, 0.00278591550886631, 0.003811623202636838, 0.004837330896407366, 0.005863038823008537, 0.0068887462839484215, 0.007914453744888306, 0.008940162137150764, 0.009965869598090649, 0.010991577059030533, 0.012017285451292992, 0.013042992912232876, 0.01406870037317276, 0.015094408765435219, 0.016120117157697678, 0.017145823687314987, 0.018171532079577446, 0.019197238609194756, 0.020222947001457214, 0.021248655393719673, 0.022274361923336983, 0.02330007031559944, 0.0243257787078619, 0.02535148523747921, 0.02637719362974167, 0.027402902022004128, 0.028428608551621437, 0.029454316943883896, 0.030480025336146355, 0.031505733728408813, 0.03253144025802612]}, '_timestamp': 1717427320.730407}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/bn1.weight': {'_type': 'histogram', 'values': [53, 4, 4, 4, 4, 3, 1, 1, 1, 4, 2, 0, 1, 2, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 3, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 1, 0, 0, 2, 1, 2, 0, 2, 0, 0, 0, 1, 0, 2, 2, 1, 1, 3, 0, 4, 1, 2, 3, 7], 'bins': [0.9993996024131775, 0.9994181990623474, 0.9994367957115173, 0.9994553923606873, 0.9994739890098572, 0.9994925856590271, 0.999511182308197, 0.9995298385620117, 0.9995484352111816, 0.9995670318603516, 0.9995856285095215, 0.9996042251586914, 0.9996228218078613, 0.9996414184570312, 0.9996600151062012, 0.9996786117553711, 0.999697208404541, 0.9997158050537109, 0.9997344017028809, 0.9997529983520508, 0.9997716546058655, 0.9997902512550354, 0.9998088479042053, 0.9998274445533752, 0.9998460412025452, 0.9998646378517151, 0.999883234500885, 0.9999018311500549, 0.9999204277992249, 0.9999390244483948, 0.9999576210975647, 0.9999762177467346, 0.9999948740005493, 1.0000134706497192, 1.0000320672988892, 1.000050663948059, 1.000069260597229, 1.000087857246399, 1.0001064538955688, 1.0001250505447388, 1.0001436471939087, 1.0001622438430786, 1.0001808404922485, 1.0001994371414185, 1.0002180337905884, 1.0002366304397583, 1.0002552270889282, 1.0002738237380981, 1.000292420387268, 1.000311017036438, 1.000329613685608, 1.0003482103347778, 1.0003669261932373, 1.0003855228424072, 1.0004041194915771, 1.000422716140747, 1.000441312789917, 1.000459909439087, 1.0004785060882568, 1.0004971027374268, 1.0005156993865967, 1.0005342960357666, 1.0005528926849365, 1.0005714893341064, 1.0005900859832764]}, '_timestamp': 1717427320.730585}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/bn1.bias': {'_type': 'histogram', 'values': [6, 3, 2, 1, 2, 12, 1, 0, 1, 0, 3, 1, 0, 1, 1, 2, 2, 0, 0, 1, 2, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 35, 0, 0, 0, 0, 0, 1, 1, 2, 0, 0, 2, 1, 2, 1, 1, 0, 1, 2, 0, 0, 2, 2, 2, 0, 0, 2, 8, 2, 2, 5, 4, 6], 'bins': [-0.0006002520676702261, -0.0005814929027110338, -0.0005627336795441806, -0.0005439745145849884, -0.0005252153496257961, -0.0005064561846666038, -0.00048769699060358107, -0.00046893779654055834, -0.00045017863158136606, -0.00043141943751834333, -0.0004126602434553206, -0.0003939010784961283, -0.0003751418844331056, -0.0003563827194739133, -0.0003376235254108906, -0.0003188643604516983, -0.00030010516638867557, -0.00028134597232565284, -0.00026258680736646056, -0.00024382762785535306, -0.00022506844834424555, -0.00020630925428122282, -0.00018755007477011532, -0.0001687908952590078, -0.0001500317157479003, -0.0001312725362367928, -0.0001125133567256853, -9.375416993862018e-05, -7.499499042751268e-05, -5.623581091640517e-05, -3.747662776731886e-05, -1.8717446437221952e-05, 4.173489287495613e-08, 1.8800916222971864e-05, 3.756009755306877e-05, 5.6319280702155083e-05, 7.507846021326259e-05, 9.383763972437009e-05, 0.00011259682651143521, 0.00013135600602254272, 0.00015011518553365022, 0.00016887436504475772, 0.00018763354455586523, 0.00020639272406697273, 0.00022515191812999547, 0.00024391109764110297, 0.0002626702771522105, 0.00028142944211140275, 0.0003001886361744255, 0.0003189478302374482, 0.0003377069951966405, 0.0003564661892596632, 0.0003752253542188555, 0.00039398454828187823, 0.0004127437132410705, 0.00043150290730409324, 0.000450262101367116, 0.00046902126632630825, 0.000487780460389331, 0.0005065396544523537, 0.000525298819411546, 0.0005440579843707383, 0.0005628171493299305, 0.0005815763724967837, 0.000600335537455976]}, '_timestamp': 1717427320.7307441}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/fc2.weight': {'_type': 'histogram', 'values': [238, 260, 238, 293, 261, 268, 230, 280, 251, 263, 249, 296, 245, 260, 241, 248, 244, 254, 282, 265, 250, 237, 265, 250, 234, 245, 278, 242, 262, 267, 260, 276, 271, 277, 276, 267, 261, 277, 263, 255, 256, 252, 255, 241, 250, 221, 255, 298, 232, 267, 210, 248, 240, 287, 261, 244, 223, 269, 264, 268, 259, 249, 227, 229], 'bins': [-0.2165754735469818, -0.20980025827884674, -0.20302505791187286, -0.1962498426437378, -0.18947464227676392, -0.18269942700862885, -0.17592422664165497, -0.1691490113735199, -0.16237381100654602, -0.15559859573841095, -0.14882339537143707, -0.142048180103302, -0.13527296483516693, -0.12849776446819305, -0.12172255665063858, -0.1149473488330841, -0.10817213356494904, -0.10139692574739456, -0.09462171792984009, -0.08784651011228561, -0.08107130229473114, -0.07429609447717667, -0.06752088665962219, -0.06074567884206772, -0.053970471024513245, -0.04719525948166847, -0.040420051664114, -0.033644843846559525, -0.02686963602900505, -0.020094426348805428, -0.013319218531250954, -0.006544009782373905, 0.0002311989665031433, 0.007006407715380192, 0.01378161646425724, 0.020556824281811714, 0.027332033962011337, 0.03410724177956581, 0.040882449597120285, 0.04765765741467476, 0.05443286895751953, 0.061208076775074005, 0.06798328459262848, 0.07475849241018295, 0.08153370022773743, 0.0883089080452919, 0.09508411586284637, 0.10185932368040085, 0.10863453149795532, 0.1154097467660904, 0.12218495458364487, 0.12896016240119934, 0.13573536276817322, 0.1425105780363083, 0.14928579330444336, 0.15606099367141724, 0.1628362089395523, 0.16961140930652618, 0.17638662457466125, 0.18316182494163513, 0.1899370402097702, 0.19671224057674408, 0.20348745584487915, 0.21026265621185303, 0.2170378714799881]}, '_timestamp': 1717427320.731008}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/fc2.bias': {'_type': 'histogram', 'values': [2, 0, 2, 5, 1, 2, 4, 0, 0, 1, 1, 1, 0, 3, 2, 3, 2, 2, 1, 1, 2, 3, 2, 3, 2, 0, 2, 1, 2, 2, 4, 3, 2, 0, 3, 2, 0, 3, 2, 3, 2, 2, 2, 2, 3, 2, 2, 2, 1, 1, 2, 1, 4, 2, 1, 2, 5, 0, 2, 3, 3, 4, 3, 3], 'bins': [-0.08825472742319107, -0.08551270514726639, -0.0827706903219223, -0.08002866804599762, -0.07728665322065353, -0.07454463094472885, -0.07180260866880417, -0.06906059384346008, -0.0663185715675354, -0.06357655674219131, -0.06083453446626663, -0.05809251591563225, -0.055350493639707565, -0.05260847508907318, -0.0498664565384388, -0.04712443798780441, -0.04438241571187973, -0.041640397161245346, -0.03889837861061096, -0.03615636005997658, -0.033414341509342194, -0.03067232109606266, -0.027930300682783127, -0.025188282132148743, -0.02244626358151436, -0.019704243168234825, -0.01696222461760044, -0.014220204204320908, -0.011478185653686523, -0.008736166171729565, -0.005994146689772606, -0.003252127207815647, -0.0005101077258586884, 0.0022319117560982704, 0.004973931238055229, 0.007715950254350901, 0.010457970201969147, 0.013199988752603531, 0.015942009165883064, 0.01868402771651745, 0.021426048129796982, 0.024168066680431366, 0.02691008523106575, 0.029652105644345284, 0.03239412605762482, 0.0351361446082592, 0.037878163158893585, 0.04062018170952797, 0.043362200260162354, 0.046104222536087036, 0.04884624108672142, 0.051588259637355804, 0.05433027818799019, 0.05707230046391487, 0.059814319014549255, 0.06255634129047394, 0.06529835611581802, 0.0680403783917427, 0.07078239321708679, 0.07352441549301147, 0.07626643776893616, 0.07900845259428024, 0.08175047487020493, 0.08449248969554901, 0.0872345119714737]}, '_timestamp': 1717427320.7311769}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/bn2.weight': {'_type': 'histogram', 'values': [11, 8, 9, 9, 10, 2, 4, 1, 2, 0, 1, 1, 0, 1, 5, 0, 1, 1, 0, 1, 2, 0, 0, 1, 2, 2, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 1, 1, 0, 2, 1, 1, 1, 0, 0, 0, 1, 1, 3, 1, 3, 1, 1, 3, 4, 2, 4, 3, 4, 10], 'bins': [0.9993998408317566, 0.9994185566902161, 0.9994373321533203, 0.9994560480117798, 0.9994747638702393, 0.9994934797286987, 0.999512255191803, 0.9995309710502625, 0.9995496869087219, 0.9995684027671814, 0.9995871782302856, 0.9996058940887451, 0.9996246099472046, 0.9996433258056641, 0.9996621012687683, 0.9996808171272278, 0.9996995329856873, 0.9997183084487915, 0.999737024307251, 0.9997557401657104, 0.9997744560241699, 0.9997932314872742, 0.9998119473457336, 0.9998306632041931, 0.9998493790626526, 0.9998681545257568, 0.9998868703842163, 0.9999055862426758, 0.9999243021011353, 0.9999430775642395, 0.999961793422699, 0.9999805092811584, 0.9999992847442627, 1.0000180006027222, 1.0000367164611816, 1.0000554323196411, 1.0000741481781006, 1.00009286403656, 1.000111699104309, 1.0001304149627686, 1.000149130821228, 1.0001678466796875, 1.000186562538147, 1.0002052783966064, 1.000223994255066, 1.0002427101135254, 1.0002615451812744, 1.0002802610397339, 1.0002989768981934, 1.0003176927566528, 1.0003364086151123, 1.0003551244735718, 1.0003738403320312, 1.0003926753997803, 1.0004113912582397, 1.0004301071166992, 1.0004488229751587, 1.0004675388336182, 1.0004862546920776, 1.000504970550537, 1.0005236864089966, 1.0005425214767456, 1.000561237335205, 1.0005799531936646, 1.000598669052124]}, '_timestamp': 1717427320.731329}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/bn2.bias': {'_type': 'histogram', 'values': [10, 6, 6, 5, 3, 9, 0, 1, 2, 1, 0, 3, 0, 1, 2, 1, 1, 1, 1, 1, 2, 0, 2, 1, 3, 0, 4, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 6, 1, 1, 2, 1, 1, 1, 1, 1, 0, 0, 3, 1, 1, 2, 5, 2, 2, 5, 6, 13], 'bins': [-0.0005997584667056799, -0.0005810277070850134, -0.0005622969474643469, -0.0005435662460513413, -0.0005248354864306748, -0.0005061047268100083, -0.0004873739671893418, -0.0004686432075686753, -0.00044991247705183923, -0.00043118171743117273, -0.0004124509578105062, -0.0003937202272936702, -0.0003749894676730037, -0.00035625870805233717, -0.0003375279775355011, -0.0003187972179148346, -0.00030006648739799857, -0.00028133572777733207, -0.00026260496815666556, -0.0002438742230879143, -0.000225143478019163, -0.0002064127183984965, -0.00018768197332974523, -0.00016895122826099396, -0.00015022046864032745, -0.00013148972357157618, -0.0001127589785028249, -9.402822615811601e-05, -7.529747381340712e-05, -5.656672874465585e-05, -3.783597639994696e-05, -1.9105227693216875e-05, -3.7447898648679256e-07, 1.835626972024329e-05, 3.708701842697337e-05, 5.581777077168226e-05, 7.454851584043354e-05, 9.327926818514243e-05, 0.00011201002052985132, 0.0001307407655986026, 0.00014947151066735387, 0.00016820227028802037, 0.00018693301535677165, 0.00020566376042552292, 0.00022439452004618943, 0.0002431252651149407, 0.000261856010183692, 0.0002805867698043585, 0.000299317529425025, 0.00031804825994186103, 0.00033677901956252754, 0.0003555097500793636, 0.0003742405097000301, 0.0003929712693206966, 0.00041170199983753264, 0.00043043275945819914, 0.00044916351907886565, 0.0004678942495957017, 0.0004866250092163682, 0.0005053557688370347, 0.0005240865284577012, 0.0005428172880783677, 0.0005615479894913733, 0.0005802787491120398, 0.0005990095087327063]}, '_timestamp': 1717427320.731478}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/fc3.weight': {'_type': 'histogram', 'values': [234, 266, 282, 252, 259, 268, 228, 278, 267, 236, 219, 273, 251, 248, 260, 229, 236, 282, 250, 237, 250, 267, 283, 250, 237, 263, 247, 230, 227, 234, 238, 262, 262, 251, 269, 256, 284, 314, 258, 244, 273, 271, 229, 228, 247, 256, 267, 257, 244, 277, 244, 253, 273, 262, 269, 245, 263, 264, 261, 269, 277, 274, 257, 243], 'bins': [-0.21687434613704681, -0.21009542047977448, -0.20331649482250214, -0.1965375691652298, -0.18975865840911865, -0.1829797327518463, -0.17620080709457397, -0.16942188143730164, -0.1626429557800293, -0.15586403012275696, -0.14908510446548462, -0.14230617880821228, -0.13552725315093994, -0.1287483423948288, -0.12196941673755646, -0.11519049108028412, -0.10841156542301178, -0.10163263976573944, -0.0948537141084671, -0.08807479590177536, -0.08129587024450302, -0.07451694458723068, -0.06773802638053894, -0.0609591007232666, -0.05418017506599426, -0.047401249408721924, -0.04062232747673988, -0.03384340554475784, -0.027064479887485504, -0.020285556092858315, -0.013506632298231125, -0.006727708503603935, 5.1215291023254395e-05, 0.006830139085650444, 0.013609062880277634, 0.020387986674904823, 0.027166910469532013, 0.03394583612680435, 0.04072475805878639, 0.04750367999076843, 0.05428260564804077, 0.06106153130531311, 0.06784045696258545, 0.07461937516927719, 0.08139830082654953, 0.08817722648382187, 0.09495614469051361, 0.10173507034778595, 0.10851399600505829, 0.11529292166233063, 0.12207184731960297, 0.1288507729768753, 0.13562968373298645, 0.1424086093902588, 0.14918753504753113, 0.15596646070480347, 0.1627453863620758, 0.16952431201934814, 0.17630323767662048, 0.18308216333389282, 0.18986108899116516, 0.1966399997472763, 0.20341892540454865, 0.21019785106182098, 0.21697677671909332]}, '_timestamp': 1717427320.7317271}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/fc3.bias': {'_type': 'histogram', 'values': [3, 3, 0, 0, 3, 4, 2, 2, 1, 3, 2, 1, 0, 0, 1, 2, 2, 1, 5, 3, 3, 0, 1, 1, 1, 2, 3, 4, 1, 2, 2, 1, 1, 2, 1, 1, 2, 1, 0, 2, 3, 3, 2, 2, 0, 3, 3, 2, 5, 2, 1, 4, 0, 1, 2, 1, 4, 1, 3, 2, 4, 2, 7, 2], 'bins': [-0.08722168207168579, -0.08448400348424911, -0.08174631744623184, -0.07900863885879517, -0.0762709528207779, -0.07353327423334122, -0.07079558819532394, -0.06805790960788727, -0.06532022356987, -0.06258254498243332, -0.059844858944416046, -0.05710717663168907, -0.0543694943189621, -0.05163181200623512, -0.04889412969350815, -0.046156447380781174, -0.0434187650680542, -0.04068108648061752, -0.03794340416789055, -0.035205721855163574, -0.0324680395424366, -0.029730355367064476, -0.0269926730543375, -0.024254990741610527, -0.021517310291528702, -0.018779627978801727, -0.016041945666074753, -0.013304262422025204, -0.010566581040620804, -0.00782889872789383, -0.005091216415166855, -0.0023535341024398804, 0.0003841482102870941, 0.0031218305230140686, 0.005859512835741043, 0.008597195148468018, 0.011334877461194992, 0.014072558842599392, 0.01681024208664894, 0.019547924399375916, 0.02228560671210289, 0.025023287162184715, 0.02776096947491169, 0.030498651787638664, 0.03323633596301079, 0.03597401827573776, 0.03871170058846474, 0.04144938290119171, 0.04418706148862839, 0.04692474380135536, 0.049662426114082336, 0.05240010842680931, 0.055137790739536285, 0.05787547305226326, 0.060613155364990234, 0.06335084140300751, 0.06608851999044418, 0.06882620602846146, 0.07156388461589813, 0.0743015706539154, 0.07703924924135208, 0.07977693527936935, 0.08251461386680603, 0.0852522999048233, 0.08798997849225998]}, '_timestamp': 1717427320.731877}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/bn3.weight': {'_type': 'histogram', 'values': [24, 5, 9, 9, 8, 2, 1, 1, 2, 2, 1, 0, 1, 0, 1, 4, 1, 1, 2, 2, 0, 1, 1, 2, 1, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 3, 3, 1, 3, 2, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 2, 2, 2, 1, 2, 1, 0, 2, 11], 'bins': [0.9993998408317566, 0.9994186162948608, 0.9994373321533203, 0.9994561076164246, 0.9994748830795288, 0.9994936585426331, 0.9995123744010925, 0.9995311498641968, 0.999549925327301, 0.9995686411857605, 0.9995874166488647, 0.999606192111969, 0.9996249079704285, 0.9996436834335327, 0.999662458896637, 0.9996812343597412, 0.9996999502182007, 0.9997187256813049, 0.9997375011444092, 0.9997562170028687, 0.9997749924659729, 0.9997937679290771, 0.9998124837875366, 0.9998312592506409, 0.9998500347137451, 0.9998688101768494, 0.9998875260353088, 0.9999063014984131, 0.9999250769615173, 0.9999437928199768, 0.999962568283081, 0.9999813437461853, 1.0000001192092896, 1.000018835067749, 1.0000375509262085, 1.0000563859939575, 1.000075101852417, 1.000093936920166, 1.0001126527786255, 1.000131368637085, 1.000150203704834, 1.0001689195632935, 1.000187635421753, 1.000206470489502, 1.0002251863479614, 1.000243902206421, 1.00026273727417, 1.0002814531326294, 1.0003001689910889, 1.000319004058838, 1.0003377199172974, 1.0003564357757568, 1.0003752708435059, 1.0003939867019653, 1.0004127025604248, 1.0004315376281738, 1.0004502534866333, 1.0004690885543823, 1.0004878044128418, 1.0005065202713013, 1.0005253553390503, 1.0005440711975098, 1.0005627870559692, 1.0005816221237183, 1.0006003379821777]}, '_timestamp': 1717427320.732026}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/bn3.bias': {'_type': 'histogram', 'values': [19, 8, 3, 5, 2, 8, 2, 2, 4, 0, 3, 0, 2, 1, 4, 2, 1, 2, 1, 2, 1, 0, 0, 0, 2, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 1, 0, 2, 2, 5, 1, 2, 3, 3, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 2, 1, 3, 3, 6, 8], 'bins': [-0.0006003946764394641, -0.0005816415068693459, -0.0005628883372992277, -0.0005441351677291095, -0.0005253819981589913, -0.0005066288285888731, -0.00048787565901875496, -0.00046912248944863677, -0.0004503693198785186, -0.0004316161503084004, -0.0004128629807382822, -0.000394109811168164, -0.0003753566415980458, -0.00035660347202792764, -0.00033785030245780945, -0.00031909713288769126, -0.00030034396331757307, -0.0002815907937474549, -0.0002628376241773367, -0.0002440844546072185, -0.00022533128503710032, -0.00020657811546698213, -0.00018782494589686394, -0.00016907177632674575, -0.00015031860675662756, -0.00013156543718650937, -0.00011281226761639118, -9.405909804627299e-05, -7.53059284761548e-05, -5.6552758906036615e-05, -3.7799589335918427e-05, -1.9046419765800238e-05, -2.932501956820488e-07, 1.845991937443614e-05, 3.721308894455433e-05, 5.596625851467252e-05, 7.47194280847907e-05, 9.34725976549089e-05, 0.00011222576722502708, 0.00013097893679514527, 0.00014973210636526346, 0.00016848527593538165, 0.00018723844550549984, 0.00020599161507561803, 0.00022474478464573622, 0.0002434979542158544, 0.0002622511237859726, 0.0002810042933560908, 0.000299757462926209, 0.00031851063249632716, 0.00033726380206644535, 0.00035601697163656354, 0.00037477014120668173, 0.0003935233107767999, 0.0004122764803469181, 0.0004310296499170363, 0.0004497828194871545, 0.00046853598905727267, 0.00048728915862739086, 0.000506042328197509, 0.0005247954977676272, 0.0005435486673377454, 0.0005623018369078636, 0.0005810550064779818, 0.0005998081760481]}, '_timestamp': 1717427320.732168}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/fc4.weight': {'_type': 'histogram', 'values': [4, 3, 1, 0, 0, 0, 4, 3, 5, 3, 2, 2, 3, 5, 2, 2, 0, 1, 1, 1, 1, 3, 1, 2, 1, 1, 3, 0, 2, 1, 3, 0, 3, 2, 2, 4, 1, 1, 3, 3, 3, 2, 2, 1, 2, 2, 1, 1, 1, 4, 1, 2, 1, 1, 0, 4, 4, 3, 3, 2, 4, 3, 0, 2], 'bins': [-0.30342426896095276, -0.29393652081489563, -0.2844488024711609, -0.27496105432510376, -0.265473335981369, -0.2559855878353119, -0.24649785459041595, -0.23701012134552002, -0.2275223731994629, -0.21803463995456696, -0.20854690670967102, -0.19905917346477509, -0.18957144021987915, -0.18008370697498322, -0.1705959588289261, -0.16110822558403015, -0.15162049233913422, -0.14213275909423828, -0.13264502584934235, -0.12315728515386581, -0.11366954445838928, -0.10418181121349335, -0.09469407796859741, -0.08520633727312088, -0.07571860402822495, -0.06623087078332901, -0.05674313008785248, -0.04725539684295654, -0.03776765987277031, -0.028279922902584076, -0.01879218779504299, -0.009304451756179333, 0.00018328428268432617, 0.009671020321547985, 0.019158756360411644, 0.02864649146795273, 0.03813422843813896, 0.047621965408325195, 0.05710969865322113, 0.06659743934869766, 0.0760851725935936, 0.08557290583848953, 0.09506064653396606, 0.104548379778862, 0.11403611302375793, 0.12352385371923447, 0.133011594414711, 0.14249932765960693, 0.15198706090450287, 0.1614747941493988, 0.17096252739429474, 0.18045027554035187, 0.1899380087852478, 0.19942574203014374, 0.20891347527503967, 0.2184012085199356, 0.22788894176483154, 0.23737668991088867, 0.2468644231557846, 0.25635215640068054, 0.26583990454673767, 0.2753276228904724, 0.28481537103652954, 0.2943030893802643, 0.3037908375263214]}, '_timestamp': 1717427320.732321}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/fc4.bias': {'_type': 'histogram', 'values': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'bins': [-0.5672603845596313, -0.5516353845596313, -0.5360103845596313, -0.5203853845596313, -0.5047603845596313, -0.48913538455963135, -0.47351038455963135, -0.45788538455963135, -0.44226038455963135, -0.42663538455963135, -0.41101038455963135, -0.39538538455963135, -0.37976038455963135, -0.36413538455963135, -0.34851038455963135, -0.33288538455963135, -0.31726038455963135, -0.30163538455963135, -0.28601038455963135, -0.27038538455963135, -0.25476038455963135, -0.23913536965847015, -0.22351036965847015, -0.20788536965847015, -0.19226036965847015, -0.17663536965847015, -0.16101036965847015, -0.14538536965847015, -0.12976036965847015, -0.11413536965847015, -0.09851036965847015, -0.08288536965847015, -0.06726036965847015, -0.051635369658470154, -0.036010369658470154, -0.020385369658470154, -0.004760369658470154, 0.010864630341529846, 0.026489630341529846, 0.042114630341529846, 0.057739630341529846, 0.07336463034152985, 0.08898963034152985, 0.10461463034152985, 0.12023963034152985, 0.13586463034152985, 0.15148963034152985, 0.16711463034152985, 0.18273963034152985, 0.19836463034152985, 0.21398963034152985, 0.22961463034152985, 0.24523963034152985, 0.26086461544036865, 0.27648961544036865, 0.29211461544036865, 0.30773961544036865, 0.32336461544036865, 0.33898961544036865, 0.35461461544036865, 0.37023961544036865, 0.38586461544036865, 0.40148961544036865, 0.41711461544036865, 0.43273961544036865]}, '_timestamp': 1717427320.73249}).